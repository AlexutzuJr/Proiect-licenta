import numpy as np
import pandas as pd
import os
import time
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.utils import to_categorical
from PIL import Image
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, precision_score, average_precision_score, precision_recall_curve, f1_score, recall_score
import seaborn as sns
from matplotlib.colors import Normalize


# incarcam fisierele CSV
train_data = pd.read_csv('train_annotations_converted.csv')
test_data = pd.read_csv('test_annotations_converted.csv')


# extragem etichetele obiectelor
train_labels = train_data['name'].values  
test_labels = test_data['name'].values 

# transformam etichetele in categorii
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)
test_labels_encoded = label_encoder.transform(test_labels)

train_labels_categorical = to_categorical(train_labels_encoded, num_classes = 20)
test_labels_categorical = to_categorical(test_labels_encoded, num_classes = 20)


# incarcam si preprocesam imaginile
def load_and_preprocess_images(image_paths, desired_size):
    images = []
    for image_path in image_paths:
        image = Image.open(image_path)
        image = image.resize(desired_size)
        image = np.array(image) / 255.0  # normalizam valorile pixelor in intervalul [0, 1]
        images.append(image)
    return np.array(images)

train_images = load_and_preprocess_images(train_data['filename'].apply(lambda x: os.path.join(r'D:\Facultate\0.Proiect licenta\1_data_preparation\data_images\train', x)).tolist(), desired_size=(100, 100))
test_images = load_and_preprocess_images(test_data['filename'].apply(lambda x: os.path.join(r'D:\Facultate\0.Proiect licenta\1_data_preparation\data_images\test', x)).tolist(), desired_size=(100, 100))


# verificam dimensiunea datelor
print("Training data size:", len(train_images))
print("Test data size:", len(test_images))


# arhitectura modelului
model = Sequential([
    Conv2D(32, (3, 3), activation = 'relu', input_shape = (100, 100, 3)),
    MaxPooling2D((2, 2)),

    Conv2D(32, (3, 3), activation = 'relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(64, activation = 'relu'),
    Dense(20, activation = 'softmax')  # stratul de iesire cu 20 de neuroni pentru clasificare
])


# compilam modelul
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


# antrenam modelul si masuram timpul de antrenare
start_train_time = time.time()
history = model.fit(train_images, train_labels_categorical, epochs = 50, batch_size = 8)
end_train_time = time.time()

# calculam media preciziei si pierderii pentru toate epocile
accuracy_values = history.history['accuracy']
loss_values = history.history['loss']

mean_accuracy = np.mean(accuracy_values)
mean_loss = np.mean(loss_values)

print("\nTrain Accuracy:", "{:.4f}".format(mean_accuracy))
print("Train Loss:", "{:.4f}".format(mean_loss))

# calculam durata de antrenare si o afisam in minute si secunde
train_duration = end_train_time - start_train_time
train_minutes = int(train_duration // 60)
train_seconds = int(train_duration % 60)
print(f"\n50 epochs completed in {train_minutes} minutes and {train_seconds} seconds.")


# evaluam modelul pe datele de testare
test_loss, test_accuracy = model.evaluate(test_images, test_labels_categorical)


# facem predictii
y_pred = model.predict(test_images)
y_test_classes = np.argmax(test_labels_categorical, axis=1)
y_pred_classes = np.argmax(y_pred, axis=1)


class_labels = ['person', 'car', 'chair', 'bottle', 'pottedplant', 'bird', 'dog', 'sofa', 'bicycle', 'horse', 'boat', 'motorbike', 'cat', 'tvmonitor', 'cow', 'sheep', 'aeroplane', 'train', 'diningtable', 'bus']

# calculam mAP@50 cu pragurile adecvate IoU
def calculate_map(y_test_classes, y_pred, iou_thresholds):
    ap_list = []
    for i in range(len(class_labels)):
        y_true = (y_test_classes == i).astype(int)
        y_scores = y_pred[:, i]
        ap_for_thresholds = []
        for iou_threshold in iou_thresholds:
            precision, recall, _ = precision_recall_curve(y_true, y_scores)
            ap = average_precision_score(y_true, y_scores)
            ap_for_thresholds.append(ap)
        ap_list.append(np.mean(ap_for_thresholds))
    return ap_list

iou_thresholds_50 = [0.5]

mAP50_per_class = calculate_map(y_test_classes, y_pred, iou_thresholds_50)


# pregatim raportul de clasificare cu valori 
report_data = []
for i, label in enumerate(class_labels):
    precision = precision_score((y_test_classes == i).astype(int), (y_pred_classes == i).astype(int), zero_division = 0)
    recall = recall_score((y_test_classes == i).astype(int), (y_pred_classes == i).astype(int), zero_division = 0)
    f1 = f1_score((y_test_classes == i).astype(int), (y_pred_classes == i).astype(int), zero_division = 0)
    support = sum(y_test_classes == i)
    report_data.append([label, support, precision, recall, mAP50_per_class[i], f1])

report_df = pd.DataFrame(report_data, columns = ['Class', 'Instances', 'P', 'R', 'mAP@50', 'F1-Score'])

# adaugam media tuturor claselor pe o linie diferita de restul
overall_precision = precision_score(y_test_classes, y_pred_classes, average = 'macro')
overall_recall = recall_score(y_test_classes, y_pred_classes, average = 'macro')
overall_f1 = f1_score(y_test_classes, y_pred_classes, average = 'macro')
overall_support = len(y_test_classes)
overall_map50 = np.mean(mAP50_per_class)

overall_row = pd.DataFrame([['all', overall_support, 
                             f"{overall_precision:.3f}", f"{overall_recall:.3f}", 
                             f"{overall_map50:.3f}", f"{overall_f1:.3f}"]], 
                           columns=['Class', 'Instances', 'P', 'R', 'mAP@50', 'F1-Score'])

report_df[['P', 'R', 'mAP@50', 'F1-Score']] = report_df[['P', 'R', 'mAP@50', 'F1-Score']].applymap(lambda x: f"{x:.3f}" if isinstance(x, float) else x)
report_df = pd.concat([overall_row, report_df], ignore_index = True)

print("Classification Report:\n")
print(report_df.to_string(index = False))


# vizualizarea progresului antrenarii
epochs_range = range(len(history.history['loss']))
blue_color = '#1f77b4'

plt.figure(figsize=(12, 10))

plt.subplot(2, 2, 1)
plt.plot(epochs_range, history.history['loss'], color = blue_color)
plt.scatter(epochs_range, history.history['loss'], color = blue_color, s = 10)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('train/loss')

plt.subplot(2, 2, 2)
plt.plot(epochs_range, history.history['accuracy'], color = blue_color)
plt.scatter(epochs_range, history.history['accuracy'], color = blue_color, s = 10)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('metrics/accuracy'); 


# matricea de confuzie
cm = confusion_matrix(y_test_classes, y_pred_classes)
cm_normalized = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]

# precizia per clasa
precision_per_class = precision_score(y_test_classes, y_pred_classes, average = None)

diagonal_values = np.diag(cm_normalized)
vertical_values = cm_normalized[:, 0].copy()

cm_normalized[:, 0] = diagonal_values
np.fill_diagonal(cm_normalized, vertical_values)

# graficul matricei de confuzie
plt.figure(figsize=(14, 10))
ax = sns.heatmap(cm_normalized, annot = False, fmt = '.2f', cmap = 'Blues', xticklabels = class_labels, yticklabels = class_labels)

norm = Normalize(vmin = 0, vmax = 1)

for i in range(cm_normalized.shape[0]):
    for j in range(cm_normalized.shape[1]):
        if cm_normalized[i, j] > 0.005: 
            color = 'white' if i == j else 'black'
            if i == j:
                background_color = plt.cm.Blues(norm(cm_normalized[i, j]))
                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill = True, color = background_color, edgecolor = 'none'))
                text = f'{precision_per_class[i]:.2f}'
                ax.text(j + 0.5, i + 0.5, text, ha = 'center', va = 'center', color = color, fontsize = 9, fontweight = 'bold')
            else:
                text = f'{cm_normalized[i, j]:.2f}'
                ax.text(j + 0.5, i + 0.5, text, ha = 'center', va = 'center', color = color, fontsize = 9)

plt.ylabel('Predicted')
plt.xlabel('True')
plt.title('Confusion Matrix')
plt.show()


# trasez curba Precision-Recall 
plt.figure(figsize = (14, 10))

average_precisions = []
all_recalls = []
all_precisions = []

for i, label in enumerate(class_labels):
    y_true = (y_test_classes == i).astype(int)
    y_scores = y_pred[:, i]
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    ap = average_precision_score(y_true, y_scores)
    average_precisions.append(ap)
    all_precisions.append(precision)
    all_recalls.append(recall)
    plt.plot(recall, precision, label = f'{label} {ap:.3f}')

# calculam si trasam curba medie Precision-Recall 
average_precision = np.mean(average_precisions)

# interpolare pentru curba medie Precision-Recall 
mean_recall = np.linspace(0, 1, 100)
mean_precision = np.zeros_like(mean_recall)

for i in range(len(all_recalls)):
    mean_precision += np.interp(mean_recall, all_recalls[i][::-1], all_precisions[i][::-1])

mean_precision /= len(all_recalls)

plt.plot(mean_recall, mean_precision, label = f'all classes {average_precision:.3f} mAP@0.5', color = 'blue', linewidth = 3)

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.xlim([0, 1])  
plt.ylim([0, 1])  
plt.title('Precision-Recall Curve')
plt.legend(loc = 'center left', bbox_to_anchor = (1, 0.5), fontsize = 'small')
plt.show()


# curba F1-confidence 
def f1_confidence_curve(y_true, y_scores, num_bins = 100):
    f1_scores = []
    confidences = np.linspace(0, 1, num_bins)
    
    for confidence in confidences:
        y_pred_conf = (y_scores >= confidence).astype(int)
        f1 = f1_score(y_true, y_pred_conf, average = 'macro')
        f1_scores.append(f1)
    
    return confidences, f1_scores

plt.figure(figsize = (14, 10))

for i, label in enumerate(class_labels):
    y_true = (y_test_classes == i).astype(int)
    y_scores = y_pred[:, i]
    confidences, f1_scores = f1_confidence_curve(y_true, y_scores)
    plt.plot(confidences, f1_scores, label=label)

all_f1_scores = np.array([f1_confidence_curve((y_test_classes == i).astype(int), y_pred[:, i])[1] for i in range(len(class_labels))])
average_f1_scores = np.mean(all_f1_scores, axis=0)
plt.plot(confidences, average_f1_scores, label=f'all classes {average_f1_scores.max():.2f} at {confidences[np.argmax(average_f1_scores)]:.3f}', color = 'blue', linewidth = 3)

plt.xlabel('Confidence')
plt.ylabel('F1')
plt.xlim([0, 1])  
plt.ylim([0, 1])  
plt.title('F1-Confidence Curve')
plt.legend(loc = 'center left', bbox_to_anchor = (1, 0.5), fontsize = 'small')
plt.show()


# curba Precision-confidence 
def precision_confidence_curve(y_true, y_scores, num_bins = 100):
    precision_scores = []
    confidences = np.linspace(0, 1, num_bins)
    
    for confidence in confidences:
        y_pred_conf = (y_scores >= confidence).astype(int)
        precision = precision_score(y_true, y_pred_conf, average = 'macro', zero_division = 0)
        precision_scores.append(precision)
    
    return confidences, precision_scores

plt.figure(figsize = (14, 10))

for i, label in enumerate(class_labels):
    y_true = (y_test_classes == i).astype(int)
    y_scores = y_pred[:, i]
    confidences, precision_scores = precision_confidence_curve(y_true, y_scores)
    plt.plot(confidences, precision_scores, label = label)

all_precision_scores = np.array([precision_confidence_curve((y_test_classes == i).astype(int), y_pred[:, i])[1] for i in range(len(class_labels))])
average_precision_scores = np.mean(all_precision_scores, axis = 0)
plt.plot(confidences, average_precision_scores, label=f'all classes {average_precision_scores.max():.2f} at {confidences[np.argmax(average_precision_scores)]:.3f}', color = 'blue', linewidth = 3)

plt.xlabel('Confidence')
plt.ylabel('Precision')
plt.xlim([0, 1])  
plt.ylim([0, 1])  
plt.title('Precision-Confidence Curve')
plt.legend(loc='center left', bbox_to_anchor = (1, 0.5), fontsize = 'small')
plt.show()


# curba Recall-confidence 
def recall_confidence_curve(y_true, y_scores, num_bins = 100):
    recall_scores = []
    confidences = np.linspace(0, 1, num_bins)
    
    for confidence in confidences:
        y_pred_conf = (y_scores >= confidence).astype(int)
        recall = recall_score(y_true, y_pred_conf, average = 'macro', zero_division = 0)
        recall_scores.append(recall)
    
    return confidences, recall_scores

plt.figure(figsize = (14, 10))

for i, label in enumerate(class_labels):
    y_true = (y_test_classes == i).astype(int)
    y_scores = y_pred[:, i]
    confidences, recall_scores = recall_confidence_curve(y_true, y_scores)
    plt.plot(confidences, recall_scores, label = label)

all_recall_scores = np.array([recall_confidence_curve((y_test_classes == i).astype(int), y_pred[:, i])[1] for i in range(len(class_labels))])
average_recall_scores = np.mean(all_recall_scores, axis = 0)
plt.plot(confidences, average_recall_scores, label = f'all classes {average_recall_scores.max():.2f} at {confidences[np.argmax(average_recall_scores)]:.3f}', color = 'blue', linewidth = 3)

plt.xlabel('Confidence')
plt.ylabel('Recall')
plt.xlim([0, 1])  
plt.ylim([0, 1])  
plt.title('Recall-Confidence Curve')
plt.legend(loc='center left', bbox_to_anchor = (1, 0.5), fontsize = 'small')
plt.show()


# curba Precision-Recall 
plt.figure(figsize = (14, 10))

average_precisions = []
all_recalls = []
all_precisions = []

for i, label in enumerate(class_labels):
    y_true = (y_test_classes == i).astype(int)
    y_scores = y_pred[:, i]
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    ap = average_precision_score(y_true, y_scores)
    average_precisions.append(ap)
    all_precisions.append(precision)
    all_recalls.append(recall)
    plt.plot(recall, precision, label = f'{label} {ap:.3f}')

average_precision = np.mean(average_precisions)

mean_recall = np.linspace(0, 1, 100)
mean_precision = np.zeros_like(mean_recall)

for i in range(len(all_recalls)):
    mean_precision += np.interp(mean_recall, all_recalls[i][::-1], all_precisions[i][::-1])

mean_precision /= len(all_recalls)

plt.plot(mean_recall, mean_precision, label = f'all classes {average_precision:.3f} mAP@0.5', color = 'blue', linewidth = 3)

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.xlim([0, 1])  
plt.ylim([0, 1])  
plt.title('Precision-Recall Curve')
plt.legend(loc = 'center left', bbox_to_anchor = (1, 0.5), fontsize = 'small')
plt.show()
