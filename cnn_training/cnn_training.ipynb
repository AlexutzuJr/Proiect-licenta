import numpy as np
import pandas as pd
import os
import time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from PIL import Image
from sklearn.preprocessing import MinMaxScaler


# incarcam fisierele CSV
train_data = pd.read_csv('train_annotations_converted.csv')
test_data = pd.read_csv('test_annotations_converted.csv')


# extragem etichetele tinta (coordonatele casetei de delimitare) din fisierele CSV
train_labels = train_data[['xmin', 'xmax', 'ymin', 'ymax']].values  
test_labels = test_data[['xmin', 'xmax', 'ymin', 'ymax']].values  

# preprocesam etichetele tinta daca este necesar (le scalam la un interval intre 0 si 1)
scaler = MinMaxScaler()
train_labels_scaled = scaler.fit_transform(train_labels)
test_labels_scaled = scaler.transform(test_labels)


# incarcam si preprocesam imaginile
def load_and_preprocess_images(image_paths, desired_size):
    images = []
    for image_path in image_paths:
        image = Image.open(image_path)
        image = image.resize(desired_size)
        image = np.array(image) / 255.0  # normalizam valorile pixelor in intervalul [0, 1]
        images.append(image)
    return np.array(images)

train_images = load_and_preprocess_images(train_data['filename'].apply(lambda x: os.path.join(r'D:\Facultate\0.Proiect licenta\1_data_preparation\data_images\train', x)).tolist(), desired_size=(100, 100))
test_images = load_and_preprocess_images(test_data['filename'].apply(lambda x: os.path.join(r'D:\Facultate\0.Proiect licenta\1_data_preparation\data_images\test', x)).tolist(), desired_size=(100, 100))


# verificam dimensiunea datelor
print("Training data size:", len(train_images))
print("Test data size:", len(test_images))


# arhitectura modelului
model = Sequential([
    Conv2D(32, (3, 3), activation = 'relu', input_shape = (100, 100, 3)),
    MaxPooling2D((2, 2)),

    Conv2D(32, (3, 3), activation = 'relu'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(64, activation = 'relu'),
    Dense(4, activation = 'sigmoid')  # stratul de iesire cu 4 neuroni pentru coordonatele casetei de delimitare
])


# compilam modelul
model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])


# antrenam modelul si masuram timpul de antrenare
start_train_time = time.time()
history = model.fit(train_images, train_labels_scaled, epochs = 50, batch_size = 8)
end_train_time = time.time()

# calculam media preciziei si pierderii pentru toate epocile
accuracy_values = history.history['accuracy']
loss_values = history.history['loss']

mean_accuracy = np.mean(accuracy_values)
mean_loss = np.mean(loss_values)

print("\nTrain Accuracy:", "{:.4f}".format(mean_accuracy))
print("Train Loss:", "{:.4f}".format(mean_loss))

# calculam durata de antrenare si o afisam in minute si secunde
train_duration = end_train_time - start_train_time
train_minutes = int(train_duration // 60)
train_seconds = int(train_duration % 60)
print(f"\n50 epochs completed in {train_minutes} minutes and {train_seconds} seconds.")


# evaluam modelul pe datele de testare
test_loss, test_accuracy = model.evaluate(test_images, test_labels_scaled)
